{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this once\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import mne\n",
    "import glob\n",
    "import HiguchiFractalDimension as hfd\n",
    "import csv\n",
    "\n",
    "import utils.logger\n",
    "from utils.experiments_classification import classify_nusvm_cross_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(utils.experiments_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patient_array_to_hfd(repetitions, window_start=None, window_size=500, step_size=1, k=7):\n",
    "    '''\n",
    "    Apply HFD to all repetitions from given patient; input shape [n_repetitions, n_channels, n_features]\n",
    "    '''\n",
    "    if window_start != None:\n",
    "        print('Calculating HFD with window start', str(window_start), 'window size', str(window_size), 'step', str(step_size))\n",
    "        return np.array([np.array([[hfd.hfd(channel[window_start:window_start+window_size:step_size], num_k=k)] for channel in repetition]) for repetition in repetitions])\n",
    "    print('Calculating HFD over entire time series')\n",
    "    return np.array([np.array([[hfd.hfd(channel, num_k=k)] for channel in repetition]) for repetition in repetitions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply HFD to datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns relevant datasets (f.e. all right-hand recordings of healthy patients) as a list of np arrays\n",
    "def get_datasets(patient_type_location, recording_type_expression):\n",
    "    if recording_type_expression != l_new:\n",
    "        sets_locations = glob.glob(patient_type_location + recording_type_expression + suffix)\n",
    "    else:\n",
    "        sets_locations = glob.glob(patient_type_location + recording_type_expression)\n",
    "    \n",
    "    sets = []\n",
    "    for path in sets_locations: \n",
    "        sets.append(mne.io.read_epochs_eeglab(path))\n",
    "        \n",
    "    return np.array(np.array([(patient._data) for patient in sets]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './../../'\n",
    "suffix = '*.set'\n",
    "\n",
    "location_healthy = root + 'data/raw/HV/*/'\n",
    "location_pain = root + 'data/raw/PP/*/'\n",
    "location_nopain = root + 'data/raw/PnP/*/'\n",
    "\n",
    "location_pwp = root + 'data_new/raw/PwP/*/'\n",
    "location_pdp = root + 'data_new/raw/PdP/*/'\n",
    "location_pnp = root + 'data_new/raw/PnP/*/'\n",
    "\n",
    "\n",
    "rh = '*_RH*'\n",
    "lh = '*_LH*'\n",
    "l_new = '*_L.set'   # NO SUFFIX\n",
    "l_old = '*_L_*'\n",
    "\n",
    "sets_healthy_rh = glob.glob(location_pnp + l_new)\n",
    "sets_healthy_rh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_rh_raw = get_datasets(location_pain, rh)\n",
    "pnp_rh_raw = get_datasets(location_nopain, rh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_rh_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_rh_raw[4][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get HFD over the set time window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_start_sec = 1\n",
    "window_size = 2\n",
    "freq = 250\n",
    "\n",
    "k = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_rh = np.array([patient_array_to_hfd(patient, window_start=window_start_sec*freq, window_size=window_size*freq, k=k) for patient in pp_rh_raw])\n",
    "pnp_rh = np.array([patient_array_to_hfd(patient, window_start=window_start_sec*freq, window_size=window_size*freq, k=k) for patient in pnp_rh_raw])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, get HFD over all windows of the specified size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_all_windows = np.array([[patient_array_to_hfd(patient, window_start=window*freq, window_size=window_size*freq, k=k) for patient in pp_rh_raw] for window in range(0,4)])\n",
    "pnp_all_windows = np.array([[patient_array_to_hfd(patient, window_start=window*freq, window_size=window_size*freq, k=k) for patient in pnp_rh_raw] for window in range(0,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_all_windows[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnp_rh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_rh[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_and_pnp = np.concatenate((pp_rh, pnp_rh))\n",
    "pp_and_pnp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_proc_method = 'HFD'\n",
    "log_dataset = 'PP/PNP-RH'\n",
    "log_db_name = 'log.db'\n",
    "\n",
    "log_notes = {'window start': window_start_sec, 'window size': window_size, 'k': k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = 0.652008\n",
    "channels = [12, 16]\n",
    "\n",
    "acc, sensitivity, specificity, avg_acc = classify_nusvm_cross_valid(pp_rh, pnp_rh, nu, channels,\n",
    "                                                                    log_db_name=log_db_name,\n",
    "                                                                    log_txt=True,\n",
    "                                                                    log_proc_method=log_proc_method,\n",
    "                                                                    log_dataset=log_dataset,\n",
    "                                                                    log_notes=log_notes,\n",
    "                                                                    log_details=True\n",
    "                                                                   )\n",
    "print('Accuracy', acc)\n",
    "print('Sensitivity', sensitivity)\n",
    "print('Specificity', specificity)\n",
    "print('Average accuracy', avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_channels=[17, 17, 24]\n",
    "nu = 0.7\n",
    "\n",
    "max_acc = {'index': 0, 'value': 0}\n",
    "for channel in range(61):        \n",
    "    accuracy, sensitivity, specificity, avg_accuracy = classify_nusvm_cross_valid(pp_rh, pnp_rh, nu, \n",
    "                                                                                  previous_channels + [channel], \n",
    "                                                                                  verbose=False,\n",
    "                                                                                  log_db_name=log_db_name,\n",
    "                                                                                  log_txt=True,\n",
    "                                                                                  log_proc_method=log_proc_method,\n",
    "                                                                                  log_dataset=log_dataset,\n",
    "                                                                                  log_notes=log_notes\n",
    "                                                                                 )\n",
    "    print(channel, accuracy, sensitivity, specificity, avg_accuracy)\n",
    "        \n",
    "    if accuracy > max_acc['value']:\n",
    "        max_acc['index'] = channel\n",
    "        max_acc['value'] = accuracy\n",
    "\n",
    "print('Max accuracy:', max_acc['index'], max_acc['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [17, 17, 24]\n",
    "\n",
    "max_acc = {'index': 0, 'value': 0}\n",
    "for param in np.arange(0.1, 0.8, 0.001):    \n",
    "    accuracy, sensitivity, specificity, avg_accuracy = classify_nusvm_cross_valid(pp_rh, pnp_rh, param,\n",
    "                                                                                  channels,\n",
    "                                                                                  verbose=False, \n",
    "                                                                                  log_db_name=log_db_name,\n",
    "                                                                                  log_txt=True,\n",
    "                                                                                  log_proc_method=log_proc_method,\n",
    "                                                                                  log_dataset=log_dataset,\n",
    "                                                                                  log_notes=log_notes\n",
    "                                                                                 ) \n",
    "                                                      \n",
    "\n",
    "    print(param, accuracy, sensitivity, specificity, avg_accuracy)\n",
    "        \n",
    "    if accuracy > max_acc['value']:\n",
    "        max_acc['index'] = param\n",
    "        max_acc['value'] = accuracy\n",
    "\n",
    "        \n",
    "print('Max accuracy:', max_acc['index'], max_acc['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
