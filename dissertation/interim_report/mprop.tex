\documentclass{mprop}
\usepackage{graphicx}

% alternative font if you prefer
%\usepackage{times}

% for alternative page numbering use the following package
% and see documentation for commands
%\usepackage{fancyheadings}


% other potentially useful packages
%\uspackage{amssymb,amsmath}
%\usepackage{url}
%\usepackage{fancyvrb}
%\usepackage[final]{pdfpages}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Exploring Machine Learning on EEG Signals for Predicting Central Neuropathic Pain in Spinal Cord Injury Patients}
\author{Cristian-Liviu Chirion}
\date{13 December 2019}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\educationalconsent
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{intro}

\subsection{Motivation}

Central Neuropathic Pain (CNP) is a pathological condition that frequently appears in patients who have previously suffered Spinal Cord Injury (SCI). It manifests itself as a recurring pain, which has been described as a feeling of burning, stabbing, or electric shock~\cite{hulsebosch_mechanisms_2009}, and is often intense enough to interfere with a patient's daily routine and sleep, often leading to mental health issues and even suicide~\cite{hulsebosch_mechanisms_2009,vuckovic_prediction_2018}.

The condition, which is estimated to affect between 40\% and 80\% of SCI patients~\cite{hulsebosch_mechanisms_2009,vuckovic_prediction_2018}, is permanent, and a cure for it has not yet been found. There are various treatments available for CNP patients which can reduce the pain to tolerable levels. Antidepressants, for example, have been shown to be beneficial in mitigating neuropathic pain~\cite{finnerup_review_2008}, but they often cause significant side effects such as drowsiness, fatigue, bladder issues, digestive issues, which can further interfere with a patient's life~\cite{finnerup_review_2008,khawam_side_2006}.

Being able to accurately predict whether a patient is likely to develop CNP in advance of the pain actually appearing would offer the opportunity to administer preventive treatments and could motivate new pharmacological studies for developing more efficient pain prevention medication.

\subsection{Prediction of CNP}

Electroencephalography (EEG) is a technique for monitoring and analysing brain activity by placing multiple sensors across a patient's scalp and using them to measure the intensity of electrical activity over time in various parts of the brain. Individual sensors are referred to as 'channels', each channel representing a different location on the scalp~\cite{noauthor_multi-channel_nodate}. There have been various studies that concluded that there are statistically significant differences between the EEG signals of SCI patients without CNP and SCI patients who have developed or are about to develop CNP~\cite{vuckovic_prediction_2018}. This provides an opportunity for researching machine learning techniques to classify SCI patients based on whether they develop pain or not, thus making it possible to predict the onset of CNP.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Statement of Problem}

The goal of this project is to create a machine learning model that can accurately classify SCI patients into one of two categories based on their EEG readings: patients who develop CNP and patients who don't. The algorithm will be trained using an existing data set of patients from both categories, consisting of the patients' EEG recordings. This data has been collected from SCI patients for a previous study, which analyzed the EEG activity of SCI patients with CNP, SCI patients without CNP, as well as able-bodied patients~\cite{vuckovic_dynamic_2014}.

For the purpose of the EEG recordings in our working data set, patients were asked to sit still while looking at a screen. A readiness cue in the shape of a cross would appear on the screen, followed by a second cue after one second. The second cue was an instruction for participants to imagine one of three possible movements - a left arrow would indicate left hand movement, a right arrow would indicate right hand movement, and a down arrow would indicate movement of the legs. Participants were asked to keep imagining these movements for 3 seconds, and these trials were repeated approximately 50 times per movement for each patient~\cite{vuckovic_dynamic_2014}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background Survey}

\subsection{Opportunity for classification}

The starting point of our research is the assumption that the differences between the EEG signals of patients with pain and patients without pain will allow us to classify the two categories using machine learning techniques. This assumption is justified by previous research where such EEG signals were analyzed and statistically significant differences between the two groups were found~\cite{vuckovic_dynamic_2014,jarjees_causality_2017}, meaning that there could be an opportunity to classify these two groups based on their EEG data.

It is important to take into account the various previous studies that have attempted to detect or predict neuropathic pain by classifying EEG signals, noting which classification algorithms have been seen to perform well on similar problems, as well as how EEG data should be processed in order to yield a highly accurate classification.

A good starting point in this sense is the analysis provided by \citet{vuckovic_dynamic_2014}, which has the clear advantage of having been performed on the same data set as the one we are working with. In this paper, the main criteria used as a basis of comparison between the different patient categories is the band power over multiple frequency bands, calculated based on the EEG readings. The study finds a statistically significant difference between the readings of patients with pain and patients without pain, noting that these differences are mostly localised within particular subsets of EEG channels, representing different locations across the patients' scalps. Consequently, we can expect that calculating the power over various frequency bands from our EEG data can produce useful features to use as input in a classifier, and that it will be useful to explore selection of the different individual channels in order to find the optimal subset of channels for classification.

\subsection{Data pre-processing}
\label{data-prep}

As with any other machine learning problem, one key aspect of our research is finding an efficient way to make the most of the available data by obtaining useful features through pre-processing. This is particularly important when it comes to classifying data sets of EEG recordings such as ours, since this type of data is often noisy and likely to contain outliers, but also because such data sets are often relatively small~\cite{lotte_review_2007}. Furthermore, EEG data consists of many readings of signal power over time, which are done over multiple channels, meaning that the resulting data set will be high-dimensional. \citet{lotte_review_2007} point out that in such situations we need to take into account the phenomenon known as the 'curse of dimensionality', meaning that the size of a data set required to train an accurate classifier increases exponentially with the number of dimensions in the training data. This means that in our problem it will likely be necessary to select a small subset of the available channels and/or to apply a pre-processing method that significantly reduces the number of dimensions while preserving any potentially useful features of the data.

There are various mathematical and statistical methods that are widely used for dimensionality reduction and thus could prove useful in our problem. One such technique is Principal Component Analysis (PCA), which reduces the number of dimensions while keeping a high variance along the axes, thus maintaining the differences between various data points~\cite{wold_principal_1987,kumar_understanding_2018}. Other methods include random projections, which involve projecting the data onto a smaller number of dimensions while maintaining data variance~\cite{bingham_random_2001}, and feature agglomeration, which uses clustering to find features that are similar to each other and merge them~\cite{wang_towards_2018}.

The issue of noisy data also needs to be addressed. EEG recordings are highly susceptible to noise from both biological factors, such as eye blinks and muscle movements, as well as external factors such as radio or electrical interference~\cite{fitzgibbon_removal_2007}. It is therefore essential to apply noise removal methods so that a classification algorithm can be trained on the useful features of the data. For this purpose, \citet{shaker_eeg_2007} proposed a pipeline of multiple operations that can be applied on EEG readings for pre-processing, including filtering out frequencies that are outside the usual range of EEG signals (0 to 30 Hz), and applying a Discrete Wavelet Transform, which has been shown to be a useful technique for noise reduction~\cite{lang_noise_1996,lang_nonlinear_1995}.

For the purpose of analyzing the EEG data, it is common to estimate the power spectral density (PSD) of the signal over different frequency bands by applying a Fast Fourier Transform (FFT) to it~\cite{al-fahoum_methods_2014}. \citet{subasi_neural_2005} point out that there are four frequency bands which mostly contain the characteristic waveforms of EEG signals: delta (up to 4 Hz), theta (4 to 8 Hz), alpha (8 to 14  Hz), and beta (14 to 30  Hz). Analyzing the power of our EEG signals based on these four frequency bands would be consistent with the approach taken by \citet{vuckovic_dynamic_2014}, who use the same four bands as a basis for their calculations, although it should be noted that their definitions of the alpha and beta bands are slightly different (8 to 12 Hz and 16 to 24 Hz respectively). Applying this technique to our data set would have the advantage of drastically reducing its dimensionality, since the thousands of initial readings in each channel would be reduced to only one value per frequency band, while generating features that have been shown to be useful in previous studies~\cite{jarjees_causality_2017,vuckovic_dynamic_2014,vuckovic_prediction_2018}.

It should be noted that any approach to data processing entails a risk of information loss if not executed in an optimal manner, and that care must be taken to preserve useful features in the data when applying transformations to it. For example, \citet{klonowski_everything_2009} points out that methods involving Fourier Transforms are not always highly accurate when applied to EEG signals, and suggests non-linear approaches such as Higuchi's fractal dimension method as an alternative. Proposed by \citet{higuchi_approach_1988}, this method involves analyzing the EEG signal in the time domain, which represents the default form of EEG data, rather than a frequency domain such as the one generated by a Fourier Transform. Although we need to acknowledge and consider its limitations, such as its sensitivity to noise and its narrow interval of possible values, Higuchi's method has been shown to yield accurate results in previous EEG analysis studies, especially if the signal had previously been split into smaller segments~\cite{kesic_application_2016}. This is, therefore, a method that we should consider when processing data for this project.

A more simplistic, but potentially worthwhile, way of handling the data would be to use the raw EEG signal, after a noise reduction algorithm has been applied to it, and feed it into a classifier without any other processing. This means that the resulting machine learning model would be trained on many signal amplitude readings over time for a subset of channels. Although this approach would have the advantage of preserving all the information from the original data, the large amount of input for each data point would entail a high risk of overfitting the classifier, and choosing an optimal subset of channels would be an essential, and potentially difficult task. Nevertheless, this approach has been successfully used in studies such as the one conducted by \citet{kaper_bci_2004}, and is worth keeping into account as an option.

When it comes to data pre-processing, it is clear that no individual method yields prefect results, thus a process of trial and error might be the best way to determine which approach is best for a particular problem. Ultimately, a good approach for our study would be to try some or all of the methods described above and to evaluate their performance in order to identify the one with the best results.
 
\subsection{Training a classifier}

A key factor in the quality of results obtained in a classification problem such as ours is choosing a classification algorithm that suits the data set we are working with. We therefore analyze a number of previous studies that involved applying machine learning classifiers to EEG data in order to find out which classifiers have been seen to perform well in problems similar to ours.

\citet{lotte_review_2007} point out that linear Support Vector Machines (SVM) is a method that has been used successfully in a number of EEG classification problems, often outperforming other classifiers, in part thanks to its ability to classify high-dimensional data and relatively small data sets. The algorithm used by SVM involves representing the training data points in a multi-dimensional vector space and finding a hyperplane that separates the data points from different classes, while maximizing the distance between the hyperplane and the data points closest to it, which are referred to as 'support vectors'~\cite{gandhi_support_2018}. Examples of studies on EEG that obtained good results from SVM classification include \citet{vuckovic_prediction_2018}, who obtained a 76\% accuracy, and \citet{kaper_bci_2004}, who obtained a 84.5\% accuracy. However, \citet{gallardo_transferable_2017} notes that SVM required a large subset of channels to perform well, which could make it more difficult and time-consuming to find an optimal subset of channels to feed into the classifier.

A method that is somewhat similar to SVM is Linear Discriminant Analysis (LDA), which is also a linear algorithm which aims to find a hyperplane that separates the two classes, the difference being that, unlike SVM, LDA achieves this by projecting the data onto vectors that maximize the distance between the two classes while minimizing variance between data points of the same class~\cite{lotte_review_2007}. Since this approach is based on the variance between data points and it results in finding a subset of representative features, it is also similar to the PCA technique described in section \ref{data-prep}, although LDA is used to find the features that best discriminate between the classes, while PCA does not take class separation into account~\cite{martinez_pca_2001}. In the study conducted by \citet{vuckovic_prediction_2018}, a classifier based on LDA achieved an accuracy of 77\%, which was slightly better than the result of SVM. However, \citet{lotte_review_2007} point out that, due to its linearity, LDA is likely to perform poorly as the input data becomes more complex, an aspect that was observed in practice by \citet{gallardo_transferable_2017}, who notes that accuracy under LDA decreased significantly as the number of channels increased. This could be a disadvantage if we find that we need to keep a large subset of channels in order to preserve useful information from the initial data set.

Another method that could prove useful in our research is the Naive Bayes Classifier (NBC), which relies on Bayes' probability theorem to calculate a 'maximum a posteriori', i.e. the class that an element is most likely to belong to based on its feature vector~\cite{rish_empirical_2001}. This type of classification algorithm has the advantage of having a low expected classification error when there are only two possible classes of data~\cite{rish_empirical_2001}, meaning that it could perform well in our problem. It has been known to perform well in many different machine learning problems~\cite{rish_empirical_2001, chen_automated_2012}, including EEG signal classification~\cite{vuckovic_prediction_2018}, often resulting in a high accuracy, although we also need to consider that NBC is not typically used with high-dimensional input data~\cite{chen_automated_2012}, and that the theory behind this classifier assumes that the input features are independent from each other, which is generally not the case in EEG data~\cite{gallardo_transferable_2017}.

Since we are working with a machine learning problem, it is important to consider the option of a deep learning algorithm that makes use of a neural network to classify the data, considering that neural networks are a modern approach which is widely used across many different domains in classification tasks. A neural network comprises multiple layers of functions called 'neurons', which perform repeated operations on the input data to calculate and predict the output. The parameters of the neurons are updated over many iterations as to maximize the accuracy over the given training data set~\cite{yiu_understanding_2019}. While neural networks have yielded good results in a number of studies involving EEG signal classification~\cite{vuckovic_prediction_2018,subasi_neural_2005,lotte_review_2007}, we need to consider their limitations, such as the risk of overfitting when the data is noisy~\cite{lotte_review_2007}, or the fact that their performance usually depends on the size of the data set~\cite{mishkin_systematic_2017}, which could be a disadvantage considering that our data set is relatively small.

The classification techniques described above are among the methods that are commonly mentioned in studies involving EEG data. However, there are other algorithms that we should also consider when classifying our data. K Nearest Neighbours (KNN), for example, is worth noting as a simple algorithm that is easy to implement and to evaluate~\cite{harrison_machine_2019}, thus it could provide a good option for initial classification attempts, where we can use it to evaluate our chosen data pre-processing technique by inputting the processed data into the classifier and evaluating its accuracy. Another algorithm that could prove useful is Logistic Regression, which has been seen to perform well on high-dimensional data sets~\cite{moore_logistic_2004}, thus is could be a good classification option if we use the EEG signal without dimensionality reduction or band power transformation.

It is clearly difficult, if not impossible, to know in advance which classification method will yield the best results on our data set - a concept known as the 'no free lunch' theorem, which states that different algorithms will have different results on different problems. The best approach will be to try multiple data pre-processing tactics, as well as multiple classifiers, and use a thorough evaluation method to find out which combination gives the best results.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed Approach}

\subsection{Data sets}

The main data set we are working on, collected by \citet{vuckovic_dynamic_2014}, consists of a sample of 19 patients with SCI, including 10 with CNP and 9 without CNP. There are 60 trials (some of which have been removed due to excessive noise) per patient for each of the movements that the patients were asked to imagine: waving the left hand, waving the right hand, and tapping their feet. Each trial consists of a 5 second EEG signal, recorded at a frequency of 250 Hz, where patients where showed a readiness cue at the beginning, and an initiation cue, instructing them to imagine a movement, after one second. The EEG signals were recorder over a total of 61 channels.

A secondary data set which was also made available to us consists of EEG signals from 14 patients, recorded before any of them had developed pain, including 7 who developed pain later on an another 7 who never developed pain. This data set was recorded in the same conditions as the main one, the only exception being that this secondary one has only 48 channels. Although the two data sets cannot be combined into one due to clinical differences between them, and the secondary data set is unlikely to be useful for training a classifier due to its reduced size, it could be useful to test a classifier on the secondary data set once it has been trained on the main one, thus giving us an idea of how the classification of patients who are about to develop pain differs from that of patients who have already developed pain.

\subsection{Data processing \& classification}

We have previously identified a set of potential data processing and feature extraction methods, including raw EEG signals, FFT band power calculation, Higuchi's fractal dimension method, and dimensionality reduction through PCA, random projections or feature agglomeration. We have also discussed a number of classifiers including SVM, LDA, KNN, Naive Bayes, Logistic Regression and Neural Networks.

The approach we will be taking to find the highest classification accuracy we can achieve will involve trying the various different combinations of data pre-processing methods and classifiers, optimizing the hyper-parameters of each classifier, and keeping track of the performance of the classifier for each experiment. A high classification accuracy yielded by one such experiment will mean that we have found a promising combination of a data pre-processing technique and classifier. A clear advantage of this approach is that, once all the experiments will have been conducted, we will have attempted a large number of possible methods for EEG signal classification, thus we should have a clear idea of how features should be extracted from EEG to facilitate classification and which classifier is best fit to deal with this data.

The order in which we conduct these experiments will take into account the difficulty of each method, since it makes sense to start with the methods that take less time to implement. For example, KNN is a simple classifier that does not require a training phase and is thus relatively quick to implement, meaning that it can be the first classifier used to evaluate any feature extraction method, while Neural Networks require a longer training phase and have many hyper-parameters that need to be optimized, and should therefore be the last classifier we experiment with.

Of course, the fact that we will have to conduct a relatively high number of experiments means that this approach could end up being too time-consuming. One aspect that will help mitigate this issue is the fact that the software implementation will be done using Python, which means that we will have access to a large number of Python-based scientific libraries that provide easy implementations for our chosen classifiers, as well as the calculations that we will require for data pre-processing.

\subsection{Evaluation}

Thorough and accurate evaluation of our classifiers is essential for finding out which combination of data pre-processing method and classifier performs best. The main evaluation method we will be using is cross-validation, an established and widely used technique, which involves removing a slice of the data set and keeping it aside for testing, while training the classifier on the rest of the data set. The data that is kept aside for testing is referred to as a 'validation sample'. The specific procedure we will be using is known as 'V-fold cross-validation', and it involves splitting the data set into multiple validation samples of approximately equal sizes, and iterating through each validation sample V, each iteration involving training the classifier on all data apart from V, then testing the classifier's performance on V~\cite{arlot_survey_2010}.

In order to adapt this procedure to the requirements of our problem, we will use each individual patient as a validation sample, which means that we will perform as many evaluations as the total number of patients for each classifier, each evaluation involving a calculation of the classification accuracy for the patient being used as a validation sample. The accuracy will be calculated across all the EEG recording repetitions from that patient, meaning that we will keep track of the proportion of repetitions that were correctly identified as coming from either a patient with pain or a patient without pain. The overall accuracy will then be calculated by averaging the results across all patients, and will provide a good indication of how well the classifier performed. We will also keep track of how many patients were correctly labeled by each classifier. For this purpose, we will assume that a patient was correctly labelled if more than half of the recordings from that patient were correctly labelled - for example, if the classifier identified 65\% of the recordings from a patient with pain as samples of pain EEG, then we will consider that the patient was correctly labeled.

\subsection{Current progress}

tba

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Work Plan}

The current aim is to finish performing the experiments described above by the end of January. This target will include having a record of the evaluation results for all the methods that we previously discussed, meaning that we will have a good idea of which approaches gave the most promising results. This will provide an opportunity to create a shortlist of data pre-processing techniques and classifiers that are worth further investigation, and the next few weeks should be spent exploring them by tweaking the algorithms (for example, by changing the hyper-parameters of classifiers) and checking if classification accuracy can be further improved.

All practical work for this study should ideally be finished by the end of February in order to allow time for reflecting on the obtained results and for finalizing the final report for this project.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% it is fine to change the bibliography style if you want
\bibliographystyle{plainnat}
\bibliography{references}
\end{document}
